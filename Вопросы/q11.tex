\section{Понятие надежности. Методология обоснования надежности криптографической защиты.}

Под надёжностью шифра понимают его стойкость, т.е. устойчивость перед атаками криптоаналитиков. 

Шеннон вводит два вида стойкости: практическую и теоретическую. При этом он берёт в рассмотрение только атаку по одному шифртексту, т.е. когда противник перехватывает одну единицу шифртекста и больше никаких дополнительных данных. 

\subsection{Общие обозначения}

Для рассмотрения стойкости шифров введём следующие общие обозначения:

\begin{itemize}
	\item $X$ -- множество всех открытых текстов для данного шифра
	\item $Y$ -- множество всех шифртекстов для данного шифра
	\item $K$ -- множество всех ключей для данного шифра
	\item $H(x)$ -- Шенноновская энтропия элементов некоторого множества $x$
\end{itemize}

\subsection{Теоретическая стойкость}

Чтобы дать определение теоретической стойкости шифра, необходимо ввести понятие условной энтропии.

Пусть у нас есть распределения вероятностей $P(X), P(Y)$ для множеств $X$ и $Y$, т.е. каждому элементу из этих множеств поставлена в соответствие некоторая вероятность его появления.

Тогда условным распределением $P(X/y)$ назовём вероятности того, что каждый элемент $x_i \in X$ после зашифрования дал конкретный шифртекст $y$. Это распределение будет состоять из вероятностей $p(x_i/y)$, каждая из которых показывает вероятность того, что конкретный открытый текст $x_i$ при зашифровании превратился в шифртекст $y_i$. Если есть распределение, можем посчитать и энтропию. В случае условного распределения это будет условная энтропия:

$$ H(X/y) = -\sum_{x \in X} p(x/y) \cdot log_2p(x/y) $$ 

Такая энтропия показывает, насколько мы, зная конкретный шифртекст $y$, не уверены в том, из какого конкретного открытого текста $x_i$ он был получен.

Чтобы обобщить это на все шифртексты, нужно усреднить эту энтропию по всем возможным шифртекстам. Тогда мы получим условную энтропию двух распределений $X$ и $Y$:

$$ H(X/Y) = -\sum_{y \in Y} \sum_{x \in X} p(y) \cdot p(x/y) \cdot log_2p(x/y) $$ 

Теперь рассмотрим, как характеризует шифр такая энтропия.

Пусть, например, $H(X/Y) = 0$. Это будет значить, что знание конкретного шифртекста $y$ не оставляет никакой неопределённости насчёт конкретного открытого текста $x$, из которого $y$ был получен. Т.е., зная шифртекст, мы можем с точностью определить открытый текст. Очевидно, это плохой шифр.

Шеннон показывает, что $H(X/Y) \leq H(X)$, т.е. максимальное возможное значение для $H(X/Y)$ есть $H(X)$. Рассмотрим, что значит равенство $H(X/Y) = H(X)$. При условии такого равенства, мы можем говорить, что знание шифртекста никак не изменило наши предположения о том, какой открытый текст был зашифрован. Такой шифр явно хорош. Более того, согласно Шеннону, такой шифр называется идеальным и возможен для реализации только в определённых условиях.

Таким образом, вводятся два критерия теоретической стойкости шифра:

\begin{enumerate}
	\item Неопределённость шифра по открытому тексту -- $H(X/Y)$
	\item Неопределённость шифра по ключу -- $H(K/Y)$
\end{enumerate}

При этом Шеннон показал следующее соотношение: 

$$ H(K/Y) = H(X) + H(K) - H(Y) $$

Неопределённость шифра по ключу определяет ответ на следующий вопрос: зная конкретный шифртекст, насколько мы не уверены в том, с помощью какого ключа он был получен?

\subsection{Практическая стойкость}

Чтобы дать определение практической стойкости шифра, следует ввести определение расстояния единственности.

Пусть есть некоторый язык $\Lambda$ и $R_{\Lambda}$ -- избыточность этого языка. 

$R_\Lambda = 1 - \frac{H_\Lambda}{log_2n}$, где $H_\Lambda$ -- избыточность языка $\Lambda$, $n$ -- длина алфавита этого языка.

В свою очередь $H_\Lambda =  \lim\limits_{r \rightarrow \infty} \frac{H_r}{r}$, где $r$ -- длина $r$-грамм языка, по которым считается энтропия. 

Тогда для шифра, шифрующего язык $\Lambda$ с длиной алфавита $n$ расстояние единственности определяется так:

$$ L_0 = \left \lceil \frac{log_2|K|}{R_\Lambda \cdot log_2n} \right \rceil $$ 

$L_0$ -- минимальная длина шифртекста, для которого только один ключ при расшифровании даст осмысленный открытый текст. Для всех шифртекстов длиной менее $L_0$ один шифртекст при расшифровании на разных ключах может давать разные осмысленные шифртексты, таким образом, вводя криптоаналитика в заблуждение насчёт того, какой из подобранных ключей верный. Важно, что вполне возможно равенство $L_0 = \infty$.

Шеннон ввёл понятие рабочей характеристики шифра $W(N)$, показывающей средний объём работы, выраженный в некотрых элементарных вычислительных операциях, который необходим, чтобы определить ключ по шифртексту длиной $N$. 

Значение $W(N)$ растёт до тех пор, пока $N$ не достигнет $L_0$, после чего уменьшается до некоторого предела. 

В качестве значения практической стойкости шифра берётся как раз это предельное значение. Т.е. значение средней работы критпоаналитика при неограниченном объёме шифртекста. Однако на практике вычислить $\lim\limits_{N \rightarrow \infty} W(N)$ достаточно сложно, поэтому вводят величину, называемую достигнутой оценкой рабочей характеристики $ W_a(\infty) = \lim\limits_{N \rightarrow \infty} W_a(N)$ -- среднюю трудоёмкость наилучшего из известных методов вскрытия данного шифра. 

Оценивать такое значение должен эксперт-криптоаналитик. При этом задаётся какая-то пороговая трудоёмкость $\Delta$, и все шифры, для которых $W_A(\infty) > \Delta$ считаются надёжными. 